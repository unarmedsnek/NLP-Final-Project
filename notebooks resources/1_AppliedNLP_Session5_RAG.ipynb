{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb4daedf",
   "metadata": {},
   "source": [
    "# Session 5 — Retrieval Augmented Generation (RAG)\n",
    "\n",
    "In this notebook, we build a **minimal RAG pipeline** using Lewis Carroll's two Alice books as our corpus as usual:\n",
    "\n",
    "- *Alice's Adventures in Wonderland*\n",
    "- *Through the Looking-Glass*\n",
    "\n",
    "> **IMPORTANT**: It is **highly recommended** to use a virtual environment for this session!  \n",
    "> The packages and downloaded models (embeddings, transformers) can easily reach over **1 GB** in size.  \n",
    "> Using a venv keeps your system clean and makes it easy to manage these large dependencies and delete them when not needed anymore.\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that enhances LLM responses by:\n",
    "1. **Retrieving** relevant information from a knowledge base (your documents)\n",
    "2. **Augmenting** the LLM prompt with this retrieved context\n",
    "3. **Generating** an answer based on both the question and the retrieved information\n",
    "\n",
    "This approach allows LLMs to answer questions about documents they weren't trained on, and reduces hallucinations by grounding responses in actual source material.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "We will:\n",
    "\n",
    "1. **Load** the two books as plain text  \n",
    "2. **Split** them into **overlapping chunks** (text segmentation)  \n",
    "3. **Create embeddings** for each chunk (convert text to vectors)  \n",
    "4. **Store** them in a **vector database (FAISS)** for efficient similarity search  \n",
    "5. **Build** a **retrieval + generation chain** to answer questions about the books  \n",
    "6. **Query** the system with natural language questions\n",
    "\n",
    "The focus is on understanding the *pipeline*, not on perfect model choices. You can swap components (embeddings, LLMs, vector stores) as needed."
   ]
  },
  {
   "cell_type": "code",
   "id": "d83a5c73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T08:41:13.461306Z",
     "start_time": "2025-11-28T08:40:59.654449Z"
    }
   },
   "source": [
    "# Core imports\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# LangChain components\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Hub for pulling prompts\n",
    "from langsmith import Client\n",
    "hub = Client()\n",
    "\n",
    "# LLM: we use Ollama (local) here to avoid API keys\n",
    "# Make sure you have installed and started Ollama, and pulled a model, e.g.:\n",
    "#   - install from https://ollama.com\n",
    "#   - in a terminal, run: `ollama pull llama3.2`\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# A small helper for nicer printing\n",
    "import textwrap"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luccamoura101/Desktop/applied-NLP-week5/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "a1220a1c",
   "metadata": {},
   "source": [
    "## Setup & Configuration\n",
    "\n",
    "### LLM Options\n",
    "\n",
    "In this notebook, we use **Ollama** for local LLM inference (no API keys required).\n",
    "\n",
    "**Alternative LLM options:**\n",
    "- **OpenAI**: `from langchain_openai import ChatOpenAI` → requires API key\n",
    "- **Groq**: `from langchain_groq import ChatGroq` → requires API key  \n",
    "- **Anthropic**: `from langchain_anthropic import ChatAnthropic` → requires API key\n",
    "- **HuggingFace**: `from langchain_huggingface import HuggingFaceEndpoint` → requires API key\n",
    "\n",
    "**To use Ollama:**\n",
    "1. Install from [https://ollama.com/download](https://ollama.com/download)\n",
    "2. Run in terminal: `ollama pull llama3.2` (or another model)\n",
    "3. Ollama runs on `localhost:11434` by default"
   ]
  },
  {
   "cell_type": "code",
   "id": "13b93f41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T08:41:13.599815Z",
     "start_time": "2025-11-28T08:41:13.463816Z"
    }
   },
   "source": [
    "# Paths to the Alice books (plain text)\n",
    "# Adjust these paths if your files live somewhere else.\n",
    "DATA_DIR = Path(\"../data\")\n",
    "WONDERLAND_PATH = DATA_DIR / \"Wonderland.txt\"\n",
    "LOOKING_GLASS_PATH = DATA_DIR / \"Looking-Glass.txt\"\n",
    "\n",
    "# Set up local LLM via Ollama\n",
    "# If you prefer Groq or OpenAI, you can swap this block for your own client.\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0.0  # Controls randomness: 0.0 = deterministic, 1.0 = creative\n",
    ")\n",
    "\n",
    "# print(\"Data directory:\", DATA_DIR.resolve())\n",
    "# print(\"Using LLM model:\", \"llama3.2 (Ollama)\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "dc00ff56",
   "metadata": {},
   "source": [
    "### Configuration Notes\n",
    "\n",
    "**Model Selection:**\n",
    "- `llama3.2`: Fast, good for local testing (3B parameters)\n",
    "- Other Ollama models: `llama3.1`, `mistral`, `phi3` (run `ollama list` to see installed models)\n",
    "\n",
    "**Temperature Setting:**\n",
    "- `temperature=0.0`: Deterministic responses (same answer every time)\n",
    "- `temperature=0.7`: More creative/varied responses\n",
    "- `temperature=1.0`: Maximum creativity (may be less factual)\n",
    "\n",
    "For RAG applications, **lower temperatures (0.0-0.3)** are recommended to keep answers focused on retrieved content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838ebd9",
   "metadata": {},
   "source": [
    "## 1. Load books\n",
    "\n",
    "We reuse the idea of the **`load_book`** helper from earlier sessions, but keep it simple:\n",
    "\n",
    "**Steps:**\n",
    "1. **Read** the text file from disk\n",
    "2. **Strip** Project Gutenberg header/footer (boilerplate text)\n",
    "3. **Return** clean text ready for processing\n",
    "\n",
    "**Why clean the text?**\n",
    "- Project Gutenberg files contain legal notices and metadata\n",
    "- These sections aren't part of the actual book content\n",
    "- Including them would pollute our embeddings with irrelevant information\n",
    "\n",
    "**Data Sources:**\n",
    "- You can use any plain text files (`.txt`)\n",
    "- For other formats: PDF → use `PyPDF2` or `pdfplumber`, DOCX → use `python-docx`"
   ]
  },
  {
   "cell_type": "code",
   "id": "498325ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T08:41:14.012102Z",
     "start_time": "2025-11-28T08:41:13.601311Z"
    }
   },
   "source": [
    "def load_book(filepath: Path, name: str) -> str:\n",
    "    # Load and roughly clean a Project Gutenberg text file.\n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Very simple cleaning: try to cut away Gutenberg boilerplate\n",
    "    start_markers = [\"CHAPTER I\", \"*** START OF\"]\n",
    "    end_markers = [\"*** END OF\", \"End of Project Gutenberg\"]\n",
    "\n",
    "    start_idx = 0\n",
    "    for marker in start_markers:\n",
    "        if marker in text:\n",
    "            start_idx = text.find(marker)\n",
    "            break\n",
    "\n",
    "    end_idx = len(text)\n",
    "    for marker in end_markers:\n",
    "        if marker in text:\n",
    "            end_idx = text.find(marker)\n",
    "            break\n",
    "\n",
    "    cleaned = text[start_idx:end_idx].strip()\n",
    "    print(f\"{name}: {len(cleaned):,} characters after cleaning\")\n",
    "    return cleaned\n",
    "\n",
    "wonderland_text = load_book(WONDERLAND_PATH, \"Alice's Adventures in Wonderland\")\n",
    "looking_glass_text = load_book(LOOKING_GLASS_PATH, \"Through the Looking-Glass\")"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File not found: ../data/Wonderland.txt",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 29\u001B[0m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(cleaned)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m characters after cleaning\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cleaned\n\u001B[0;32m---> 29\u001B[0m wonderland_text \u001B[38;5;241m=\u001B[39m \u001B[43mload_book\u001B[49m\u001B[43m(\u001B[49m\u001B[43mWONDERLAND_PATH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mAlice\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43ms Adventures in Wonderland\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m looking_glass_text \u001B[38;5;241m=\u001B[39m load_book(LOOKING_GLASS_PATH, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThrough the Looking-Glass\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[3], line 4\u001B[0m, in \u001B[0;36mload_book\u001B[0;34m(filepath, name)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mload_book\u001B[39m(filepath: Path, name: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;66;03m# Load and roughly clean a Project Gutenberg text file.\u001B[39;00m\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m filepath\u001B[38;5;241m.\u001B[39mexists():\n\u001B[0;32m----> 4\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFile not found: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilepath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(filepath, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m      7\u001B[0m         text \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mread()\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: File not found: ../data/Wonderland.txt"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "60cc3da0",
   "metadata": {},
   "source": [
    "## 2. Chunk the texts for retrieval\n",
    "\n",
    "Large documents are **too long** to embed and retrieve as a single vector.  \n",
    "Instead, we split the books into **overlapping chunks**:\n",
    "\n",
    "### Parameters Explained\n",
    "\n",
    "- **`chunk_size`**: Maximum number of characters per chunk (default: 800)\n",
    "  - This is a **hard limit** - chunks won't exceed this size\n",
    "  - Too small → loses context, more chunks to search\n",
    "  - Too large → less precise retrieval, may exceed embedding model limits\n",
    "  - **Typical range**: 500-1500 characters\n",
    "\n",
    "- **`chunk_overlap`**: How much neighboring chunks overlap (default: 150)\n",
    "  - Ensures sentences near boundaries aren't split awkwardly\n",
    "  - Helps maintain context across chunk boundaries\n",
    "  - **Typical range**: 10-20% of chunk_size\n",
    "\n",
    "- **`separators`**: Priority order for splitting points\n",
    "  - These determine **where** to split when approaching the chunk_size limit\n",
    "  - The splitter tries each separator in order to find a natural break point:\n",
    "    1. `\\n\\n` → paragraph breaks (preferred - most context preserved)\n",
    "    2. `\\n` → line breaks\n",
    "    3. `. ` → sentence endings\n",
    "    4. ` ` → word boundaries (last resort)\n",
    "  - **Key point**: The splitter builds chunks up to ~800 chars, then looks for the best separator to split on\n",
    "\n",
    "### How It Works Together\n",
    "\n",
    "Example: If text reaches 780 characters, the splitter looks for the first `\\n\\n` (paragraph break). If found, it splits there (even if only 750 chars). If not found, it tries `\\n`, then `. `, then ` `. This keeps chunks **under 800 chars** while breaking at **natural boundaries**.\n",
    "\n",
    "### Experimentation\n",
    "\n",
    "Try adjusting these values to see how they affect:\n",
    "- Number of chunks created\n",
    "- Retrieval quality\n",
    "- Answer accuracy"
   ]
  },
  {
   "cell_type": "code",
   "id": "b86b7254",
   "metadata": {},
   "source": [
    "def chunk_text(text: str, book_name: str, chunk_size: int = 800, chunk_overlap: int = 150):\n",
    "    # Split a long text into overlapping chunks using RecursiveCharacterTextSplitter.\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],  # try to keep chunks on sentence/paragraph boundaries\n",
    "    )\n",
    "    docs = splitter.create_documents([text])\n",
    "    print(f\"{book_name}: {len(docs)} chunks (chunk_size={chunk_size}, overlap={chunk_overlap})\")\n",
    "    return docs\n",
    "\n",
    "wonderland_chunks = chunk_text(wonderland_text, \"Wonderland\")\n",
    "looking_glass_chunks = chunk_text(looking_glass_text, \"Looking-Glass\")\n",
    "\n",
    "# Combine chunks from both books into a single corpus\n",
    "all_chunks = wonderland_chunks + looking_glass_chunks\n",
    "print(\"Total chunks in corpus:\", len(all_chunks))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11fc3f1d",
   "metadata": {},
   "source": [
    "## 3. Create embeddings & build a vector database (FAISS)\n",
    "\n",
    "### What are embeddings? (We've been using them since session 3)\n",
    "\n",
    "**Embeddings** convert text into numerical vectors (arrays of numbers) that capture semantic meaning:\n",
    "- Similar texts → similar vectors\n",
    "- Enables mathematical similarity comparisons\n",
    "- Typical dimensions: 384, 768, or 1536 numbers per chunk\n",
    "\n",
    "### Vector Database (FAISS)\n",
    "\n",
    "**FAISS** (Facebook AI Similarity Search) is a library for efficient similarity search:\n",
    "- Stores all chunk embeddings\n",
    "- Quickly finds the most similar chunks to a query\n",
    "- Works entirely offline (no API needed)\n",
    "\n",
    "### Embedding Model Options\n",
    "\n",
    "**Current**: `sentence-transformers/all-mpnet-base-v2`\n",
    "- Dimensions: 768\n",
    "- Quality: High for general-purpose tasks\n",
    "- Speed: Medium\n",
    "\n",
    "**Alternatives:**\n",
    "- `all-MiniLM-L6-v2` → Faster, smaller (384 dim), slightly lower quality (you've already used this one)\n",
    "- `all-mpnet-base-v1` → Similar to v2\n",
    "- OpenAI embeddings → `text-embedding-3-small` (requires API key)\n",
    "\n",
    "**To change**: Just replace the `model_name` parameter in `HuggingFaceEmbeddings()`"
   ]
  },
  {
   "cell_type": "code",
   "id": "04cd13e7",
   "metadata": {},
   "source": [
    "VECTOR_DB_DIR = Path(\"../vector_databases\")\n",
    "VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VECTOR_DB_PATH = VECTOR_DB_DIR / \"vector_db_alice\"\n",
    "\n",
    "def create_embedding_vector_db(chunks, db_path: Path):\n",
    "    # 1. Instantiate an embedding model (HuggingFace embeddings)\n",
    "    # 2. Create a FAISS vector store from the chunks\n",
    "    # 3. Save it locally so we can reload it later\n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n",
    "\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding\n",
    "    )\n",
    "\n",
    "    vectorstore.save_local(str(db_path))\n",
    "    print(f\"Vector database saved to: {db_path}\")\n",
    "\n",
    "create_embedding_vector_db(all_chunks, VECTOR_DB_PATH)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "93044d97",
   "metadata": {},
   "source": [
    "### Performance Notes\n",
    "\n",
    "**First run:**\n",
    "- Downloads the embedding model (~420MB for all-mpnet-base-v2)\n",
    "- Creates embeddings for all chunks (may take 1-2 minutes)\n",
    "- Saves the vector database to disk\n",
    "\n",
    "**Subsequent runs:**\n",
    "- Model is cached locally\n",
    "- Can skip this step if vector database already exists\n",
    "- Just load the saved database (next section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a98fc2",
   "metadata": {},
   "source": [
    "## 4. Build a retriever from the vector database\n",
    "\n",
    "To use RAG, we need a **retriever** object that:\n",
    "\n",
    "1. Takes a user question  \n",
    "2. Converts it to an embedding (using the same model as the chunks)\n",
    "3. Finds the **k most similar chunks** in the vector store using cosine similarity\n",
    "4. Returns those chunks to be passed to the LLM\n",
    "\n",
    "### The `k` Parameter\n",
    "\n",
    "**`k=4`** means \"retrieve the 4 most similar chunks\"\n",
    "\n",
    "**Trade-offs:**\n",
    "- **Low k (1-3)**: Faster, more focused, but might miss relevant information\n",
    "- **Medium k (4-6)**: Balanced approach (recommended starting point)\n",
    "- **High k (7-15)**: More comprehensive, but may include irrelevant chunks and slow down the LLM\n",
    "\n",
    "**Experiment:** Try different `k` values to see how they affect answer quality and response time.\n",
    "\n",
    "### Search Strategies\n",
    "\n",
    "FAISS supports different search algorithms:\n",
    "- **Similarity search** (default): Returns top-k most similar chunks (we use this one here on this notebook)\n",
    "- **MMR** (Maximum Marginal Relevance): Returns diverse results\n",
    "- **Similarity with score threshold**: Only returns chunks above a certain similarity score"
   ]
  },
  {
   "cell_type": "code",
   "id": "f7fcbe1b",
   "metadata": {},
   "source": [
    "def load_retriever(db_path: Path, k: int = 4):\n",
    "    # Reload the FAISS vector store from disk and create a retriever.\n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n",
    "\n",
    "    vectorstore = FAISS.load_local(\n",
    "        folder_path=str(db_path),\n",
    "        embeddings=embedding,\n",
    "        allow_dangerous_deserialization=True,  # needed in some environments\n",
    "    )\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "    print(f\"Retriever ready (k={k}) from {db_path}\")\n",
    "    return retriever\n",
    "\n",
    "alice_retriever = load_retriever(VECTOR_DB_PATH, k=4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "45c0fc6f",
   "metadata": {},
   "source": [
    "## 5. Connect retriever + LLM = RAG chain\n",
    "\n",
    "We now create a **retrieval chain** using **LCEL** (LangChain Expression Language):\n",
    "\n",
    "### Pipeline Flow\n",
    "\n",
    "1. **Input** → User's question\n",
    "2. **Retriever** → Fetches relevant chunks from vector database\n",
    "3. **Format** → Combines chunks into context string\n",
    "4. **Prompt** → Creates LLM prompt with context + question\n",
    "5. **LLM** → Generates answer based on context\n",
    "6. **Output Parser** → Extracts clean string from LLM response\n",
    "\n",
    "### Custom Prompt Design\n",
    "\n",
    "Our prompt instructs the LLM to:\n",
    "- Use only the provided context (retrieved chunks)\n",
    "- **Cite specific passages** from the books\n",
    "- Include brief quotes to support answers\n",
    "- Avoid making up information not in the context\n",
    "\n",
    "### Prompt Customization Options\n",
    "\n",
    "You can modify the system message to change LLM behavior:\n",
    "- Add stricter citation requirements\n",
    "- Request different answer formats (bullet points, summaries, etc.)\n",
    "- Specify answer length constraints\n",
    "- Add domain-specific instructions"
   ]
  },
  {
   "cell_type": "code",
   "id": "d118fe19",
   "metadata": {},
   "source": [
    "def build_rag_chain(retriever):\n",
    "    # Connects the retriever with an LLM using a custom prompt that asks for references.\n",
    "    \n",
    "    # Custom prompt that instructs the LLM to cite sources\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful assistant answering questions about Lewis Carroll's Alice books.\n",
    "Use the following context to answer the question. Always cite specific passages from the books in your answer.\n",
    "When you use information from the context, include a brief quote or reference to show where it came from.\n",
    "\n",
    "Context:\n",
    "{context}\"\"\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    \n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    # Build RAG chain using LCEL (LangChain Expression Language)\n",
    "    # This dictionary creates two inputs for the prompt:\n",
    "    # - \"context\": runs retriever, gets docs, formats them as string → fills {context} placeholder\n",
    "    # - \"input\": passes user's question through unchanged → fills {input} placeholder\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "alice_rag_chain = build_rag_chain(alice_retriever)\n",
    "print(\"RAG chain ready.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7aeef065",
   "metadata": {},
   "source": [
    "### Alternative Prompt Strategies\n",
    "\n",
    "**Without citations** (original hub prompt):\n",
    "```python\n",
    "prompt = hub.pull_prompt(\"langchain-ai/retrieval-qa-chat\")\n",
    "```\n",
    "\n",
    "**With structured output:**\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Answer in this format:\n",
    "    ANSWER: [your answer]\n",
    "    SOURCES: [relevant quotes]\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "```\n",
    "\n",
    "**With confidence levels:**\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Answer the question and rate your confidence (low/medium/high) \n",
    "    based on how well the context supports your answer.\"\"\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1140543",
   "metadata": {},
   "source": [
    "## 6. Ask questions about our books\n",
    "\n",
    "Now we can **chat with the corpus**!\n",
    "\n",
    "### How It Works\n",
    "\n",
    "When you ask a question:\n",
    "1. Question → embedding vector\n",
    "2. Vector database → finds 4 most similar chunks\n",
    "3. Chunks + question → sent to LLM as context\n",
    "4. LLM → generates answer with citations\n",
    "5. Answer → displayed with text wrapping\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- The LLM **does NOT answer from its pretraining alone**\n",
    "- It first retrieves relevant chunks from the *Alice* books\n",
    "- Answers are **grounded in the actual text**\n",
    "- Citations help verify the information\n",
    "\n",
    "### Evaluation Tips\n",
    "\n",
    "When testing your RAG system, consider:\n",
    "- **Relevance**: Does the answer address the question?\n",
    "- **Accuracy**: Is the information correct per the source?\n",
    "- **Citation quality**: Are quotes/references provided?\n",
    "- **Completeness**: Does it cover all relevant aspects?\n",
    "- **No hallucination**: Does it avoid making up information?\n",
    "\n",
    "Try questions that:\n",
    "- Require specific details (names, events)\n",
    "- Need synthesis across multiple passages\n",
    "- Ask about comparisons between the books\n",
    "- Test the system's limits (questions not answerable from the text)"
   ]
  },
  {
   "cell_type": "code",
   "id": "e939af60",
   "metadata": {},
   "source": [
    "def ask_alice(question: str, chain=alice_rag_chain):\n",
    "    # Send a question to the RAG chain and print a nicely wrapped answer.\n",
    "    print(f\"\\nQUESTION:\\n{question}\\n\" + \"-\"*80)\n",
    "    answer = chain.invoke(question)\n",
    "    print(\"\\nANSWER:\\n\")\n",
    "    print(textwrap.fill(answer, width=100))\n",
    "\n",
    "# Example questions\n",
    "ask_alice(\"How does Alice feel when she falls down the rabbit hole?\")\n",
    "ask_alice(\"What differences are there between Wonderland and the world behind the looking-glass?\")\n",
    "ask_alice(\"How is Alice referred to in both books?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1419194f",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [FAISS Documentation](https://faiss.ai/)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [Ollama Models](https://ollama.com/library)\n",
    "- [RAG Survey Paper](https://arxiv.org/abs/2312.10997)"
   ]
  },
  {
   "cell_type": "code",
   "id": "75442753",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
